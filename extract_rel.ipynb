{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mimport_modules.py\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     exec(f\u001b[39m.\u001b[39;49mread())\n",
      "File \u001b[0;32m<string>:4\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydot'"
     ]
    }
   ],
   "source": [
    "with open(\"import_modules.py\") as f:\n",
    "    exec(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('books_new_modified_1348.csv')\n",
    "df2 = pd.read_csv('wiki_extended_divided_44654.csv')\n",
    "df3 = pd.read_csv('essays_ext_3828.csv')\n",
    "df_tot = pd.concat((df1, df2, df3), ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://localhost:8000/getExtraction'\n",
    "headers = {'Content-type': 'text/plain'}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman_reg = re.compile('(?<=^)M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})(?=$)')\n",
    "\n",
    "def romanToInt(s):\n",
    "    translations = {\n",
    "        \"I\": 1,\n",
    "        \"V\": 5,\n",
    "        \"X\": 10,\n",
    "        \"L\": 50,\n",
    "        \"C\": 100,\n",
    "        \"D\": 500,\n",
    "        \"M\": 1000\n",
    "    }\n",
    "    number = 0\n",
    "    s = s.replace(\"IV\", \"IIII\").replace(\"IX\", \"VIIII\")\n",
    "    s = s.replace(\"XL\", \"XXXX\").replace(\"XC\", \"LXXXX\")\n",
    "    s = s.replace(\"CD\", \"CCCC\").replace(\"CM\", \"DCCCC\")\n",
    "    for char in s:\n",
    "        number += translations[char]\n",
    "    return number\n",
    "\n",
    "def get_matches(text):\n",
    "    tokenize = nltk.word_tokenize(text)\n",
    "    dict = {}\n",
    "    for token in tokenize:\n",
    "        if roman_reg.match(token):\n",
    "            dict[token] = romanToInt(token)\n",
    "    return dict\n",
    "\n",
    "def replace_matches(text):\n",
    "    matches = get_matches(text)\n",
    "    for key in matches:\n",
    "        text = text.replace(key, str(matches[key]))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline with spacy to substitute pronouns \n",
    "df = pd.read_csv('new_example.csv')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "d = {}\n",
    "for i in range(len(df)):\n",
    "    doc = nlp(df['text'][i])\n",
    "    d[df['title'][i]] = doc._.coref_resolved\n",
    "new_df = pd.DataFrame(list(d.items()),columns = ['title','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenization(df):\n",
    "    out = [sent_tokenize(par) for par in df.text]\n",
    "    final=[]\n",
    "    for par in out:\n",
    "        clean = []\n",
    "        for i in range(len(par)):\n",
    "            if len(par[i]) > 5:\n",
    "                clean.append(par[i])\n",
    "        final.append(clean)\n",
    "    return final\n",
    "\n",
    "def relation_extraction(out):\n",
    "    issues = 0\n",
    "    #out = sentence_tokenization(df)\n",
    "    rel_ext = []\n",
    "    for par in out:\n",
    "        for s in par[:100]:\n",
    "            s = unidecode(s.replace('Â ', ' ').replace('/', ' '))\n",
    "            s = s.replace('$', '').replace('. . .', '. ').replace('...', '. ')\n",
    "            s = replace_matches(s)\n",
    "            s = s.replace(' the ', ' ')\n",
    "            req = requests.request('POST', url, headers=headers, data = s.encode('utf-8'))\n",
    "            try:\n",
    "                js = req.json()\n",
    "                rel_ext.extend(js)\n",
    "            except:\n",
    "                issues +=1\n",
    "                #print('JSON issue')\n",
    "    return issues, rel_ext\n",
    "\n",
    "def create_rel_dataframe(rel):\n",
    "    #issues, rel = relation_extraction(df)\n",
    "    l  = []\n",
    "    for r in rel:\n",
    "        try:\n",
    "            l.append((r['confidence'], r['sentence'], r['extraction']['arg1']['text'], r['extraction']['rel']['text'], \n",
    "                    r['extraction']['arg2s'][0]['text'], r['extraction']['negated'], r['extraction']['passive']))\n",
    "        except IndexError:\n",
    "            pass \n",
    "    \n",
    "    l = list(dict.fromkeys(l))\n",
    "    return pd.DataFrame(l, columns= ['confidence', 'sentence', 'arg1', 'rel', 'arg2', 'negated', 'passive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "out = sentence_tokenization(df_tot)\n",
    "random.shuffle(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues, rel = relation_extraction(out)\n",
    "print(f'N. Issues: {issues}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_df = create_rel_dataframe(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_df = rel_df[rel_df['arg2'].apply(lambda x: len(x.split(' ')) > 1)].reset_index(drop = True)\n",
    "rel_df = rel_df[rel_df['rel'].apply(lambda x: len(x.split(' ')) < 6)].reset_index(drop = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
