{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imported\n"
     ]
    }
   ],
   "source": [
    "with open(\"import_modules.py\") as f:\n",
    "    exec(f.read())\n",
    "\n",
    "from graph import GraphViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from joblib import load\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "main_pos_tags = {'Noun': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "                 'Pronoun': ['PRP', 'PRP$'],\n",
    "                 'Adjective': ['JJ', 'JJR', 'JJS'],\n",
    "                 'Verb': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
    "                 'Adverb': ['RB', 'RBR', 'RBS'],\n",
    "                 'Determiner': ['DT', 'PDT', 'WDT'],\n",
    "                 'Numeral': ['CD']\n",
    "                }\n",
    "\n",
    "def count_main_pos_tags(sentence):\n",
    "    pos_tags = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    tag_counts = {tag_group: 0 for tag_group in main_pos_tags}\n",
    "    for word, tag in pos_tags:\n",
    "        for tag_group, tags in main_pos_tags.items():\n",
    "            if tag in tags:\n",
    "                tag_counts[tag_group] += 1\n",
    "    return tag_counts\n",
    "\n",
    "def for_class(df,x):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    db=df[[x]]\n",
    "    db[\"length\"]=db[x].apply(lambda x: len(word_tokenize(x)))\n",
    "    db['main_pos_counts'] = db[x].apply(count_main_pos_tags)\n",
    "    db = pd.concat([db.drop('main_pos_counts', axis=1), db['main_pos_counts'].apply(pd.Series)], axis=1)\n",
    "    db[\"rel_nouns\"]=db[\"Noun\"]/db[\"length\"]\n",
    "    db[\"rel_pronouns\"]=db[\"Pronoun\"]/db[\"length\"]\n",
    "    db[\"rel_adjectives\"]=db[\"Adjective\"]/db[\"length\"]\n",
    "    db[\"rel_verbs\"]=db[\"Verb\"]/db[\"length\"]\n",
    "    db[\"rel_adverbs\"]=db[\"Adverb\"]/db[\"length\"]\n",
    "    db[\"rel_determiners\"]=db[\"Determiner\"]/db[\"length\"]\n",
    "    db[\"rel_numerals\"]=db[\"Numeral\"]/db[\"length\"]\n",
    "    db.loc[:, x] = db.loc[:, x].apply(lambda x: model.encode(x))\n",
    "    embeddings_arg1 = np.vstack(db.loc[:, x].values)\n",
    "    len_arg1 = np.expand_dims(db.loc[:, 'length'].values, axis=1)\n",
    "    nouns=np.expand_dims(db.loc[:, 'Noun'].values, axis=1)\n",
    "    pronouns=np.expand_dims(db.loc[:, 'Pronoun'].values, axis=1)\n",
    "    adjectives=np.expand_dims(db.loc[:, 'Adjective'].values, axis=1)\n",
    "    verbs=np.expand_dims(db.loc[:, 'Verb'].values, axis=1)\n",
    "    rel_nouns=np.expand_dims(db.loc[:, 'rel_nouns'].values, axis=1)\n",
    "    rel_pronouns=np.expand_dims(db.loc[:, 'rel_pronouns'].values, axis=1)\n",
    "    rel_adjectives=np.expand_dims(db.loc[:, 'rel_adjectives'].values, axis=1)\n",
    "    rel_verbs=np.expand_dims(db.loc[:, 'rel_verbs'].values, axis=1)\n",
    "    rel_adverbs=np.expand_dims(db.loc[:, 'rel_adverbs'].values, axis=1)\n",
    "    rel_determiners=np.expand_dims(db.loc[:, 'rel_determiners'].values, axis=1)\n",
    "    rel_numerals=np.expand_dims(db.loc[:, 'rel_numerals'].values, axis=1)\n",
    "    db= np.hstack([embeddings_arg1, len_arg1, nouns, pronouns, adjectives, verbs, rel_nouns, rel_pronouns, rel_adjectives, rel_verbs, rel_adverbs, rel_determiners, rel_numerals])\n",
    "    \n",
    "    return db\n",
    "\n",
    "\n",
    "def apply_model(X,model):\n",
    "    return model.predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "def merge_concepts(df):\n",
    "    #compute similarity matrix between subjects\n",
    "    nodes = df['arg1'].unique()\n",
    "    similarity_matrix = cosine_similarity(vectorizer.fit_transform(nodes))\n",
    "    \n",
    "    #create clusters of similar subjects\n",
    "    set_list = []\n",
    "    for i in range(len(nodes)):\n",
    "        s = set([nodes[j] for j in range(len(nodes)) if i != j if similarity_matrix[i][j] > 0.3])\n",
    "        s.add(nodes[i])\n",
    "        set_list.append(s)\n",
    "    for i in range(len(set_list)):\n",
    "        for j in range(i+1, len(set_list)):\n",
    "            if set_list[i] & set_list[j]:\n",
    "                set_list[i] = set_list[i] | set_list[j]\n",
    "                set_list[j] = set()\n",
    "    set_list = [s for s in set_list if s]\n",
    "    \n",
    "    #create dictionary to substitute subjects with the simpliest one in the cluster\n",
    "    dict = {}\n",
    "    for s in set_list:\n",
    "        simplest_concept = min(s, key=lambda x: len(x))\n",
    "        for concept in s:\n",
    "            dict[concept] = simplest_concept\n",
    "    \n",
    "    #substitute subjects in dataframe\n",
    "    df.arg1= df.arg1.map(dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates, subphrases, and similar phrases\n",
    "def filter_rows(df):\n",
    "    seen = set()\n",
    "    for i, row in df.iterrows():\n",
    "        phrase = row['arg1'] + ' ' + row['rel'] + ' ' + row['arg2']\n",
    "        if phrase in seen:\n",
    "            df = df.drop(i)\n",
    "        else:\n",
    "            seen.add(phrase)\n",
    "    df = df.reset_index(drop=True)\n",
    "    for l1 in list(seen):\n",
    "        for l2 in list(seen):\n",
    "            if l1 != l2 and l1 in l2:\n",
    "                try:\n",
    "                    seen.remove(l1)\n",
    "                except KeyError:\n",
    "                    continue\n",
    "    df = df[(df['arg1'] + ' ' + df['rel'] + ' ' + df['arg2']).isin(seen)].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def remove_similar_phrases(df):\n",
    "    rem_ind = set()\n",
    "    df['phrase'] = df['arg1'] + ' ' + df['rel'] + ' ' + df['arg2']\n",
    "    sim = cosine_similarity(vectorizer.fit_transform(df['phrase']))\n",
    "    for i in range(len(sim)):\n",
    "        for j in range(i+1, len(sim)):\n",
    "            if sim[i][j] > 0.30:\n",
    "                if len((df['phrase'].unique()[i]).split(' ')) > len((df['phrase'].unique()[j]).split(' ')):\n",
    "                    rem_ind.add(j)\n",
    "                else:\n",
    "                    rem_ind.add(i)\n",
    "    return df.drop(rem_ind).reset_index(drop = True).drop('phrase', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_COMM = {}\n",
    "def concat_objects(relation):\n",
    "    object_list = relation['arg2'].tolist()\n",
    "    max_conf = np.max(relation['confidence'])\n",
    "    common = ''\n",
    "    first = object_list[0].split(' ')\n",
    "    for i in range(len(first)):\n",
    "        if all(obj.startswith(common) for obj in object_list):\n",
    "            if common == '':\n",
    "                common += first[i] \n",
    "            elif i < len(first) - 1:\n",
    "                common += ' ' + first[i]\n",
    "        else:\n",
    "            common = ' '.join(common.split(' ')[:-1])\n",
    "            break\n",
    "    common = common.rstrip()  \n",
    "    # remove trailing spaces\n",
    "    if common == '':\n",
    "        return max_conf, ', '.join(object_list)\n",
    "    objects = [obj[len(common):].lstrip() for obj in object_list]\n",
    "    if all(obj == '' for obj in objects):\n",
    "        return max_conf, (common).rstrip()\n",
    "    if len(objects) == 1:\n",
    "        return max_conf, (common + ' ' + objects[0]).rstrip()\n",
    "    DICT_COMM[common] = objects\n",
    "    return max_conf, (common + ' ' + ', '.join(objects[:-1]) + ' and ' + objects[-1]).rstrip()\n",
    "\n",
    "#Concatenate subjects with 'and' where there is the same object and relation\n",
    "def concat_subjects(relation):\n",
    "    subject_list = relation['arg1'].tolist()\n",
    "    max_conf = np.max(relation['confidence'])\n",
    "    return max_conf, (' and '.join(subject_list)).rstrip()\n",
    "\n",
    "def concat_concepts(df):\n",
    "    result = df.groupby(['arg1', 'rel']).apply(concat_objects).reset_index()\n",
    "    result['confidence'], result['arg2'] = zip(*result[0])\n",
    "    result = result.drop(columns = 0).sort_values(by='confidence', ascending=False).reset_index(drop=True)\n",
    "    result = result.groupby(['arg2', 'rel']).apply(concat_subjects).reset_index()\n",
    "    result['confidence'], result['arg1'] = zip(*result[0])\n",
    "    result = result.drop(columns = 0).sort_values(by='confidence', ascending=False).reset_index(drop=True)\n",
    "    result = result[['confidence', 'arg1', 'rel', 'arg2']]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmap_pipeline(df, min_conf_level, index):\n",
    "    DICT_COMM.clear()\n",
    "    df = df[df['confidence'] >= df[\"confidence\"].quantile(min_conf_level)]\n",
    "    df = merge_concepts(df)\n",
    "    df = filter_rows(df)\n",
    "    df = concat_concepts(df)\n",
    "    df = df[df['arg2'].apply(lambda x: len(x.split(' ')) > 1)].reset_index(drop = True)\n",
    "    df = df[df['rel'].apply(lambda x: len(x.split(' ')) < 6)].reset_index(drop = True)\n",
    "    df = remove_similar_phrases(df)\n",
    "    G = GraphViz(df)\n",
    "    G.graph.write_png(f'example_graph_{index}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load('model_file.joblib')\n",
    "model2 = load('model_file_2.joblib')\n",
    "def cmap_pipeline(df, min_conf_level, index):\n",
    "    DICT_COMM.clear()\n",
    "    df = df[df['confidence'] >= df[\"confidence\"].quantile(min_conf_level)]\n",
    "    input=for_class(df,\"arg1\")\n",
    "    input2=for_class(df,\"arg2\")\n",
    "    res=apply_model(input,model)\n",
    "    res2=apply_model(input2,model2)\n",
    "    df = df.loc[(res==1)&(res2==1)]\n",
    "    df = merge_concepts(df)\n",
    "    df = filter_rows(df)\n",
    "    df = concat_concepts(df)\n",
    "    df = df[df['arg2'].apply(lambda x: len(x.split(' ')) > 1)].reset_index(drop = True)\n",
    "    df = df[df['rel'].apply(lambda x: len(x.split(' ')) < 6)].reset_index(drop = True)\n",
    "    df = remove_similar_phrases(df)\n",
    "    G = GraphViz(df)\n",
    "    G.graph.write_png(f'example_graph_{index}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#The first one works, the second one doesn't\n",
    "cmap_pipeline(data['Reykjavík Summit'], 0.5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#The first one works, the second one doesn't\n",
    "cmap_pipeline(data['Nazi Party Rallies'], 0.60, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sembra ci sia un problema con qualche carattere speciale nel testo, ma è solo nel salvare l'immagine che da errore (write_png)\n",
    "# Ho provato a sostituire graphviz and pydot con networkx + graphviz_layout (lo portavano come soluzione del problema su stackoverflow), \n",
    "# ed è dove ho avuto l'altro errore. \n",
    "# THANK YOU! ;) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
