{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imported\n"
     ]
    }
   ],
   "source": [
    "with open(\"import_modules.py\") as f:\n",
    "    exec(f.read())\n",
    "\n",
    "from graph import GraphViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from joblib import load\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "def prepreocessing(final):\n",
    "    final=final.copy()\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    final.loc[:, 'arg1'] = final.loc[:, 'arg1'].apply(lambda x: model.encode(x))\n",
    "    final.loc[:, 'rel'] = final.loc[:, 'rel'].apply(lambda x: model.encode(x))\n",
    "    final.loc[:, 'arg2'] = final.loc[:, 'arg2'].apply(lambda x: model.encode(x))\n",
    "    embeddings_arg1 = np.vstack(final.loc[:, 'arg1'].values)\n",
    "    embeddings_rel = np.vstack(final.loc[:, 'rel'].values)\n",
    "    embeddings_arg2 = np.vstack(final.loc[:, 'arg2'].values)\n",
    "    # final.loc[:, 'negated']=final.loc[:, 'negated'].apply(lambda x: 1 if x==True else 0)\n",
    "    # final.loc[:, 'passive']=final.loc[:, 'passive'].apply(lambda x: 1 if x==True else 0)\n",
    "    # negated = np.expand_dims(final.loc[:, 'negated'].values, axis=1)\n",
    "    # passive = np.expand_dims(final.loc[:, 'passive'].values, axis=1)\n",
    "    confidence= final.loc[:, ['confidence']].values\n",
    "    # X = np.hstack([embeddings_arg1, embeddings_rel, embeddings_arg2, negated, passive, confidence])\n",
    "    X = np.hstack([embeddings_arg1, embeddings_rel, embeddings_arg2, confidence])\n",
    "    return X\n",
    "\n",
    "\n",
    "def apply_model(X):\n",
    "    model = load('model_file.joblib')\n",
    "    return model.predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "def merge_concepts(df):\n",
    "    #compute similarity matrix between subjects\n",
    "    nodes = df['arg1'].unique()\n",
    "    similarity_matrix = cosine_similarity(vectorizer.fit_transform(nodes))\n",
    "    \n",
    "    #create clusters of similar subjects\n",
    "    set_list = []\n",
    "    for i in range(len(nodes)):\n",
    "        s = set([nodes[j] for j in range(len(nodes)) if i != j if similarity_matrix[i][j] > 0.3])\n",
    "        s.add(nodes[i])\n",
    "        set_list.append(s)\n",
    "    for i in range(len(set_list)):\n",
    "        for j in range(i+1, len(set_list)):\n",
    "            if set_list[i] & set_list[j]:\n",
    "                set_list[i] = set_list[i] | set_list[j]\n",
    "                set_list[j] = set()\n",
    "    set_list = [s for s in set_list if s]\n",
    "    \n",
    "    #create dictionary to substitute subjects with the simpliest one in the cluster\n",
    "    dict = {}\n",
    "    for s in set_list:\n",
    "        simplest_concept = min(s, key=lambda x: len(x))\n",
    "        for concept in s:\n",
    "            dict[concept] = simplest_concept\n",
    "    \n",
    "    #substitute subjects in dataframe\n",
    "    df.arg1= df.arg1.map(dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates, subphrases, and similar phrases\n",
    "def filter_rows(df):\n",
    "    seen = set()\n",
    "    for i, row in df.iterrows():\n",
    "        phrase = row['arg1'] + ' ' + row['rel'] + ' ' + row['arg2']\n",
    "        if phrase in seen:\n",
    "            df = df.drop(i)\n",
    "        else:\n",
    "            seen.add(phrase)\n",
    "    df = df.reset_index(drop=True)\n",
    "    for l1 in list(seen):\n",
    "        for l2 in list(seen):\n",
    "            if l1 != l2 and l1 in l2:\n",
    "                try:\n",
    "                    seen.remove(l1)\n",
    "                except KeyError:\n",
    "                    continue\n",
    "    df = df[(df['arg1'] + ' ' + df['rel'] + ' ' + df['arg2']).isin(seen)].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def remove_similar_phrases(df):\n",
    "    rem_ind = set()\n",
    "    df['phrase'] = df['arg1'] + ' ' + df['rel'] + ' ' + df['arg2']\n",
    "    sim = cosine_similarity(vectorizer.fit_transform(df['phrase']))\n",
    "    for i in range(len(sim)):\n",
    "        for j in range(i+1, len(sim)):\n",
    "            if sim[i][j] > 0.30:\n",
    "                if len((df['phrase'].unique()[i]).split(' ')) > len((df['phrase'].unique()[j]).split(' ')):\n",
    "                    rem_ind.add(j)\n",
    "                else:\n",
    "                    rem_ind.add(i)\n",
    "    return df.drop(rem_ind).reset_index(drop = True).drop('phrase', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_COMM = {}\n",
    "def concat_objects(relation):\n",
    "    object_list = relation['arg2'].tolist()\n",
    "    max_conf = np.max(relation['confidence'])\n",
    "    common = ''\n",
    "    first = object_list[0].split(' ')\n",
    "    for i in range(len(first)):\n",
    "        if all(obj.startswith(common) for obj in object_list):\n",
    "            if common == '':\n",
    "                common += first[i] \n",
    "            elif i < len(first) - 1:\n",
    "                common += ' ' + first[i]\n",
    "        else:\n",
    "            common = ' '.join(common.split(' ')[:-1])\n",
    "            break\n",
    "    common = common.rstrip()  \n",
    "    # remove trailing spaces\n",
    "    if common == '':\n",
    "        return max_conf, ', '.join(object_list)\n",
    "    objects = [obj[len(common):].lstrip() for obj in object_list]\n",
    "    if all(obj == '' for obj in objects):\n",
    "        return max_conf, (common).rstrip()\n",
    "    if len(objects) == 1:\n",
    "        return max_conf, (common + ' ' + objects[0]).rstrip()\n",
    "    DICT_COMM[common] = objects\n",
    "    return max_conf, (common + ' ' + ', '.join(objects[:-1]) + ' and ' + objects[-1]).rstrip()\n",
    "\n",
    "#Concatenate subjects with 'and' where there is the same object and relation\n",
    "def concat_subjects(relation):\n",
    "    subject_list = relation['arg1'].tolist()\n",
    "    max_conf = np.max(relation['confidence'])\n",
    "    return max_conf, (' and '.join(subject_list)).rstrip()\n",
    "\n",
    "def concat_concepts(df):\n",
    "    result = df.groupby(['arg1', 'rel']).apply(concat_objects).reset_index()\n",
    "    result['confidence'], result['arg2'] = zip(*result[0])\n",
    "    result = result.drop(columns = 0).sort_values(by='confidence', ascending=False).reset_index(drop=True)\n",
    "    result = result.groupby(['arg2', 'rel']).apply(concat_subjects).reset_index()\n",
    "    result['confidence'], result['arg1'] = zip(*result[0])\n",
    "    result = result.drop(columns = 0).sort_values(by='confidence', ascending=False).reset_index(drop=True)\n",
    "    result = result[['confidence', 'arg1', 'rel', 'arg2']]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cmap_pipeline(df, min_conf_level, index):\n",
    "#     DICT_COMM.clear()\n",
    "#     df = df[df['confidence'] >= df[\"confidence\"].quantile(min_conf_level)]\n",
    "#     df = merge_concepts(df)\n",
    "#     df = filter_rows(df)\n",
    "#     df = concat_concepts(df)\n",
    "#     df = df[df['arg2'].apply(lambda x: len(x.split(' ')) > 1)].reset_index(drop = True)\n",
    "#     df = df[df['rel'].apply(lambda x: len(x.split(' ')) < 6)].reset_index(drop = True)\n",
    "#     df = remove_similar_phrases(df)\n",
    "#     G = GraphViz(df)\n",
    "#     G.graph.write_png(f'example_graph_{index}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmap_pipeline(df, min_conf_level, index):\n",
    "    DICT_COMM.clear()\n",
    "    df = df[df['confidence'] >= df[\"confidence\"].quantile(min_conf_level)]\n",
    "    input=prepreocessing(df)\n",
    "    res=apply_model(input)\n",
    "    df = df.loc[res==1]\n",
    "    df = merge_concepts(df)\n",
    "    df = filter_rows(df)\n",
    "    df = concat_concepts(df)\n",
    "    df = df[df['arg2'].apply(lambda x: len(x.split(' ')) > 1)].reset_index(drop = True)\n",
    "    df = df[df['rel'].apply(lambda x: len(x.split(' ')) < 6)].reset_index(drop = True)\n",
    "    df = remove_similar_phrases(df)\n",
    "    G = GraphViz(df)\n",
    "    G.graph.write_png(f'example_graph_{index}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#The first one works, the second one doesn't\n",
    "cmap_pipeline(data['Reykjavík Summit'], 0.75, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first one works, the second one doesn't\n",
    "cmap_pipeline(data['Palestinian Expatriates'], 0.82, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sembra ci sia un problema con qualche carattere speciale nel testo, ma è solo nel salvare l'immagine che da errore (write_png)\n",
    "# Ho provato a sostituire graphviz and pydot con networkx + graphviz_layout (lo portavano come soluzione del problema su stackoverflow), \n",
    "# ed è dove ho avuto l'altro errore. \n",
    "# THANK YOU! ;) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
